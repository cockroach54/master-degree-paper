{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"My dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, class_path, transform=None):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.class_path = class_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # get image list\n",
    "        all_list =  [os.path.splitext(f) for f in os.listdir(self.root_dir)] # jpg + txt\n",
    "        assert [f for f,e in all_list if e=='.jpg'] == [f for f,e in all_list if e=='.txt'], \"num_image != num_txt\"\n",
    "        self.data_list = [f for f,e in all_list if e=='.jpg']\n",
    "        \n",
    "        # get class list\n",
    "        with open(class_path, 'r', encoding='utf-8') as f:\n",
    "            self.classes = [c for c in f.read().split('\\n')]\n",
    "        self.class_to_idx = {self.classes[i]: i for i in range(len(self.classes))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.data_list[idx]+'.jpg')\n",
    "        ann_path = os.path.join(self.root_dir, self.data_list[idx]+'.txt')\n",
    "        img = Image.open(image_path)\n",
    "        with open(ann_path, 'r') as f:\n",
    "            anno = [a.split(' ') for a in f.read().strip().split('\\n')]\n",
    "        anno = np.array(anno)\n",
    "        labels = anno[:,0].astype(np.int)\n",
    "        bboxes_yolo = anno[:,1:].astype(np.float) # centerX, centerY, w, h [ratio] - yolo style\n",
    "        bboxes = bboxes_yolo.copy() # x,y,w,h\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img).unsqueeze(0)\n",
    "            \n",
    "        bboxes[:,0] = (bboxes_yolo[:,0] - bboxes_yolo[:,2]/2)*img.shape[3] # x\n",
    "        bboxes[:,1] = (bboxes_yolo[:,1] - bboxes_yolo[:,3]/2)*img.shape[2] # y\n",
    "        bboxes[:,2] = bboxes_yolo[:,2]*img.shape[3]\n",
    "        bboxes[:,3] = bboxes_yolo[:,3]*img.shape[2]\n",
    "        bboxes = bboxes.astype(np.int)\n",
    "\n",
    "        return img, labels, bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import time\n",
    "from time import sleep\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# import mymodels\n",
    "# from mymodels.BN_Inception import BN_Inception\n",
    "\n",
    "def my_cos(x,y):\n",
    "    _x = F.normalize(x, p=2, dim=1)\n",
    "    _y = F.normalize(y, p=2, dim=1)\n",
    "    return _x.matmul(_y.transpose(0,1))\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    torch.nn.Sequential에서 사용가능한 flatten 모듈\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "class OracleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(OracleModel, self).__init__()\n",
    "#         ****torch pretrained net****\n",
    "#         net = models.shufflenetv2.shufflenet_v2_x1_0(pretrained=True) # 1024\n",
    "#         net = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\n",
    "#         net = models.mobilenet_v2(pretrained=True)\n",
    "#         net = models.densenet201(pretrained=True)\n",
    "        net = models.resnet50(pretrained=True)\n",
    "\n",
    "        modules = list(net.children())[:-2]      # resnet conv_5\n",
    "        self.backbone = torch.nn.Sequential(*modules) # 2048\n",
    "        avg_pool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        flatten = Flatten() \n",
    "        \n",
    "#         net = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, \n",
    "#                                                              pretrained_backbone=True)   \n",
    "#         self.backbone = net.backbone #256 \n",
    "\n",
    "\n",
    "# #         ****mobilenet****\n",
    "#         self.BACKBONE_PATH = 'torch_models/mobile_ft.pt'\n",
    "#         mobilenet = torch.load(self.BACKBONE_PATH)\n",
    "#         modules = list(mobilenet.children())[:-1]\n",
    "#         avg_pool = torch.nn.AvgPool2d(7, stride=1)\n",
    "#         flatten = Flatten()\n",
    "#         self.backbone = torch.nn.Sequential(*modules, avg_pool, flatten) #1280\n",
    "        \n",
    "# #         ****mobilnet distance matric learning****   \n",
    "#         self.BACKBONE_PATH = 'mymodels/model_000400.pth'\n",
    "#         self.backbone = torch.load(self.BACKBONE_PATH) #1280    \n",
    "\n",
    "#         self.backbone = mymodels.create('Resnet50', dim=512, pretrained=True,\n",
    "#                                         model_path='mymodels/ckp_ep210.pth.tar') #[227,227]->512\n",
    "\n",
    "#         self.backbone = BN_Inception(dim=512, pretrained=True, \n",
    "#                                      model_path='mymodels/bn_inception-52deb4733.pth')\n",
    "\n",
    "        # fc layer\n",
    "        self.fc = nn.Sequential(\n",
    "            avg_pool,\n",
    "            flatten\n",
    "        ) # 2048\n",
    "        \n",
    "#         self.embed = nn.Sequential(\n",
    "#             self.backbone,\n",
    "#             self.fc\n",
    "#         ) # 2048\n",
    "    \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.topk = 3\n",
    "        self.threshold = 0.75\n",
    "        self.feature_len = 2048\n",
    "\n",
    "        self.sort_order_descending = False\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.roi_upsample = nn.UpsamplingBilinear2d([224,224])\n",
    "        self.roi_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "        \n",
    "    def embed(self, images):\n",
    "        r = self(images)\n",
    "        return self.fc(r)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        return self.backbone(images)\n",
    "#         return self.backbone(images)[2]    \n",
    "    \n",
    "\n",
    "    def makeAllReference_online(self, referenceImgDirPath):\n",
    "        \"\"\"\n",
    "        임베딩 디비 생성... flask 서버용 \n",
    "        utils.makeAllReferenceCSV 에서 csv 저장만 안함\n",
    "        \"\"\"\n",
    "        reference_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=referenceImgDirPath,\n",
    "            transform=transforms.Compose([\n",
    "#                 transforms.Resize([224,224]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        )\n",
    "        reference_loader = torch.utils.data.DataLoader(\n",
    "            reference_dataset,\n",
    "            batch_size=32,\n",
    "            num_workers=0,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # get all data and input to model\n",
    "        _temp = []\n",
    "        for data, target in tqdm(reference_loader):\n",
    "            outputs = self.embed(data.to(self.device)).data # .data 안하면 메모리 오버플로남...\n",
    "            _temp.append(outputs)\n",
    "\n",
    "        represented = torch.cat(_temp, dim=0)\n",
    "        # raw data label 별 평균 구해두기\n",
    "        self.df_ref_online = pd.DataFrame(represented.cpu().numpy())\n",
    "        self.df_ref_online['label'] = [reference_dataset.classes[i] for i in reference_dataset.targets]\n",
    "        self.reference_means_online = self.df_ref_online.groupby('label').mean()\n",
    "        self.dir_list_online = reference_dataset.classes\n",
    "\n",
    "        # 즉각 임베딩 디비 생성\n",
    "        self.setReferenceDataset(self.dir_list_online, self.df_ref_online, self.reference_means_online)\n",
    "        return self.dir_list_online, self.df_ref_online, self.reference_means_online\n",
    "\n",
    "    def addNewData_online(self, im_arr, label):\n",
    "        \"\"\"\n",
    "        @params im_arr: h X w X 3 (RGB)\n",
    "        @params label: \"praL\"\n",
    "        \"\"\"\n",
    "        data = self.roi_transform(im_arr).unsqueeze(0)\n",
    "        outputs = self.embed(data.to(self.device)).data\n",
    "        new_feature = list(outputs.squeeze().cpu().numpy())\n",
    "        new_feature.append(label)\n",
    "        new_feature[-1]\n",
    "        # concat to prev df_ref_online\n",
    "        self.df_ref_online.loc[self.df_ref_online.shape[0]] = new_feature\n",
    "        # revise reference_means_online \n",
    "        self.reference_means_online = self.df_ref_online.groupby('label').mean()\n",
    "        # 즉각 임베딩 디비 생성\n",
    "        self.setReferenceDataset(self.dir_list_online, self.df_ref_online, self.reference_means_online)\n",
    "        print('[Detector]: addNewData_online -', label)\n",
    "        return\n",
    "\n",
    "    def addNewLabel_online(self, dirPath, label):   \n",
    "        \"\"\"\n",
    "        @params dirPath: \"server/vu-visor/static/images_ext\"\n",
    "        @params label: \"praL\"\n",
    "        \"\"\"\n",
    "        finalDirPath = os.path.join(dirPath, label)\n",
    "        ims = [Image.open(os.path.join(finalDirPath,i)) for i in os.listdir(finalDirPath)] # n_img X h X w X c\n",
    "        ims_tensor = torch.stack([self.roi_transform(im) for im in ims])\n",
    "        outputs = self.embed(ims_tensor.to(self.device)).data\n",
    "        new_features =outputs.cpu().numpy() # n_img X 2048\n",
    "\n",
    "        new_df_ref_online = pd.DataFrame(new_features)\n",
    "        new_df_ref_online['label'] = label\n",
    "        # add new label\n",
    "        if label not in self.dir_list_online: # 중복된 이름 없을 시 새로 등록\n",
    "            self.dir_list_online.append(label)\n",
    "            self.dir_list_online.sort() # 반드시 소팅해서 클래스 번호 재정렬 해줘야함\n",
    "        # concat to prev df_ref_online\n",
    "        _df_ref_online_non_overlap = self.df_ref_online[self.df_ref_online['label']!=label] # 중복된 이전 데이터 제거\n",
    "        self.df_ref_online = pd.concat([_df_ref_online_non_overlap, new_df_ref_online], ignore_index=True)\n",
    "        # revise reference_means_online \n",
    "        self.reference_means_online = self.df_ref_online.groupby('label').mean()\n",
    "        # 즉각 임베딩 디비 생성\n",
    "        self.setReferenceDataset(self.dir_list_online, self.df_ref_online, self.reference_means_online)\n",
    "        print('[Detector]: addNewLabel_online -', label)\n",
    "        return \n",
    "        \n",
    "    # 임베딩 디비 생성\n",
    "    def setReferenceDataset(self, sample_dir_list, df_ref_feature_sampled, reference_means_sampled):\n",
    "        self.reference_classes = sample_dir_list\n",
    "        self.reference_targets = list(df_ref_feature_sampled.iloc[:,-1])\n",
    "        self.embedded_features_cpu = torch.tensor(df_ref_feature_sampled.iloc[:,:-1].as_matrix()).float().data # float64->float32(torch default), cpu\n",
    "        self.embedded_features = self.embedded_features_cpu.to(self.device) # gpu\n",
    "        self.embedded_means_numpy = reference_means_sampled.as_matrix()\n",
    "        self.embedded_means = torch.tensor(self.embedded_means_numpy).float().data.to(self.device) # float64->float32(torch default), gpu\n",
    "\n",
    "        self.c2i = {c:i for i,c in enumerate(self.reference_classes)}\n",
    "        self.embedded_labels = torch.tensor([self.c2i[c] for c in self.reference_targets]).to(self.device)\n",
    "        \n",
    "        # make datafames for plot\n",
    "        # ***필요없음 어차피 pca 해야함\n",
    "        self.df_ = pd.DataFrame(np.array(self.embedded_features_cpu))\n",
    "        self.df_['name'] = self.reference_targets\n",
    "        self.centers_ = pd.DataFrame(self.embedded_means_numpy)\n",
    "        self.centers_['name'] = self.reference_classes\n",
    "\n",
    "    def fit_pca(self, n_components=4):\n",
    "        # pca model fit\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=self.n_components) # 2048 차원 다쓰면 나중에 샘플링에서 계산 오류남, sample 개수보다 많으면 촐레스키 분해 에러나는듯\n",
    "        self.transformed = self.pca.fit_transform(self.embedded_features_cpu)\n",
    "\n",
    "        # show PCA features \n",
    "        self.df = pd.DataFrame(self.transformed)\n",
    "        self.df['name'] = self.reference_targets\n",
    "        self.centers = pd.DataFrame(self.pca.transform(self.embedded_means_numpy))\n",
    "        self.centers['name'] = self.reference_classes\n",
    "\n",
    "    def inference_tensor3(self, inputs, metric='cos', knn=True):\n",
    "        \"\"\"\n",
    "        roi 피처맵을(nxcx7x7) 인풋으로 받음\n",
    "        @params metric: [l2, mahalanobis, prob]\n",
    "        @params inputs: # N x C x H x W\n",
    "        \"\"\"            \n",
    "        # inputs shape: Batch*C*H*W\n",
    "        # input to backbone model\n",
    "        self.inputs = inputs\n",
    "        self.outputs = self.fc(self.inputs).data # n_roi X features 2048    \n",
    "\n",
    "        # inference\n",
    "        self.sort_order_descending = True         \n",
    "        if knn: self.dists = my_cos(self.outputs, self.embedded_features)\n",
    "        else: self.dists = my_cos(self.outputs, self.embedded_means)\n",
    "\n",
    "        self.dists_sorted = self.dists.sort(dim=1, descending=self.sort_order_descending)\n",
    "        if knn: self.predicts = self.embedded_labels[self.dists_sorted.indices][:,:self.topk]\n",
    "        else: self.predicts = self.dists_sorted.indices[:,:self.topk]\n",
    "\n",
    "        self.predicts_dist = self.dists_sorted.values[:,:self.topk]\n",
    "        return self.predicts.data, self.predicts_dist.data    \n",
    "\n",
    "    def inference_tensor_texture(self, inputs, metric='cos', knn=True, SUM=True):\n",
    "        \"\"\"\n",
    "        현재 미니배치 1인 경우만 작동\n",
    "        roi 피처맵을(nxcx7x7) 인풋으로 받음\n",
    "        @params metric: [l2, mahalanobis, prob]\n",
    "        @params inputs: # N x C x H x W\n",
    "        \"\"\"            \n",
    "        # inputs shape: Batch*C*H*W\n",
    "        # input to backbone model\n",
    "        # 1 X 2048 X 7 X 7 --> 7*7 X 2048 --> 1 X 3\n",
    "        # n X 2048 X 7 X 7 --> n*7*7 X 2048 --> n X 3\n",
    "        self.outputs = inputs.data.transpose(1,2).transpose(2,3).reshape(-1,inputs.shape[1])\n",
    "        # self.outputs = inputs.squeeze().transpose(0,1).transpose(1,2).reshape(-1,inputs.shape[1]).data \n",
    "        # 7*7 X features 2048   \n",
    "        \n",
    "        # inference\n",
    "        self.sort_order_descending = True         \n",
    "        if knn: self.dists = my_cos(self.outputs, self.embedded_features)\n",
    "        else: self.dists = my_cos(self.outputs, self.embedded_means)\n",
    "\n",
    "        self.dists_sorted = self.dists.sort(dim=1, descending=self.sort_order_descending)\n",
    "        if knn: self.predicts = self.embedded_labels[self.dists_sorted.indices][:,:self.topk]\n",
    "        else: self.predicts = self.dists_sorted.indices[:,:self.topk]\n",
    "\n",
    "        self.predicts_dist = self.dists_sorted.values[:,:self.topk]\n",
    "\n",
    "        if(SUM):\n",
    "    #             ---- dist sum mode ----\n",
    "            aa = pd.DataFrame(list(zip(self.predicts[:,0].cpu().numpy(), self.predicts_dist[:,0].cpu().numpy())), columns=['label','dist'])\n",
    "            self.aa = aa\n",
    "            # pp = aa.groupby('label').sum()\n",
    "            # pp = pp.sort_values(by='dist', ascending=False)\n",
    "            # pp = list(pp.index[:3])\n",
    "            # if(len(pp)!=self.topk):\n",
    "            #     pp = pp*self.topk\n",
    "            #     pp = pp[:self.topk]\n",
    "\n",
    "            # 배치별 번호 매기기\n",
    "            self.aa['batch'] =  np.repeat(np.arange(inputs.shape[0]), inputs.shape[2]*inputs.shape[3])\n",
    "            # 다중 그루핑\n",
    "            zz = self.aa.groupby(['batch', 'label']).agg(['sum', 'mean', 'max'])\n",
    "            # 2중 컬럼 인덱스 해제\n",
    "            zz.columns = zz.columns.droplevel(0) \n",
    "            # sum 순으로 정렬\n",
    "            zz = zz.reset_index().sort_values(by=['batch', 'sum'], ascending=False).set_index(['batch', 'label'])\n",
    "            # 배치별 가장 큰 값만 리턴, dist는 선택 레이블의 평균값\n",
    "            preds = zz.groupby('batch').head(1).sort_index(0).index.get_level_values(level=1).tolist()\n",
    "            preds = np.repeat(preds, 3).reshape(-1,3)\n",
    "            return torch.tensor(preds).data\n",
    "        else:\n",
    "    #             ---- voting mode ----\n",
    "            c = Counter(torch.cat([*self.predicts]).cpu().numpy())\n",
    "            self.c = c\n",
    "            pp = list(zip(*(c.most_common()[:3])))[0]\n",
    "            if(len(pp)!=self.topk):\n",
    "                pp = pp*self.topk\n",
    "                pp = pp[:self.topk]    \n",
    "            return torch.tensor([pp]).data      \n",
    "\n",
    "    \n",
    "    def inference_tensor4(self, inputs):\n",
    "        \"\"\"\n",
    "        roi 피처맵을(nxcx7x7) 인풋으로 받음\n",
    "        \"\"\"            \n",
    "        # inputs shape: Batch*C*H*W\n",
    "        # input to backbone model\n",
    "        self.inputs = inputs\n",
    "        self.outputs = self.fc(self.inputs).data # n_roi X features 2048    \n",
    "\n",
    "        # inference\n",
    "        self.dists = my_cos(self.outputs, self.embedded_means)\n",
    "        self.predicts_dist = self.dists\n",
    "        return self.predicts_dist.data    \n",
    "\n",
    "    # 각 레이블 별 평균과의 L2 거리 계산 (PCA 적용안함)\n",
    "    def calc_l2(self, label):\n",
    "        mu = self.embedded_means[label]\n",
    "        xx = self.outputs\n",
    "\n",
    "        # 20*1280 X 1280*20 --diag--> 20\n",
    "        # num_roi*dim X dim*num_roi --diag--> num_roi\n",
    "        xx_sub_mu = xx.sub(mu)\n",
    "        l2_dist = xx_sub_mu.matmul(xx_sub_mu.t()).sqrt().diag()\n",
    "        return l2_dist.cpu()\n",
    "\n",
    "    # 각 레이블 별 평균과의 cosine simility 계산 (PCA 적용안함)\n",
    "    def calc_cos(self, label):\n",
    "        mu = self.embedded_means[label]\n",
    "        xx = self.outputs\n",
    "\n",
    "        # 20*1280 X 1280*20 --diag--> 20\n",
    "        # num_roi*dim X dim*num_roi --diag--> num_roi\n",
    "        mu = mu.unsqueeze(0)\n",
    "        cos_dist = self.cos(xx, mu)\n",
    "        return cos_dist.cpu()        \n",
    "\n",
    "    def inference_file(self, imgPath):              \n",
    "        # imgPath = \"./server/oracle_proj/predict.jpg\"\n",
    "        frame = cv2.imread(os.path.join(imgPath), cv2.IMREAD_COLOR)\n",
    "        return self.inference_tensor(frame)\n",
    "\n",
    "    def show_img(self, imgPath):\n",
    "        frame = cv2.imread(os.path.join(imgPath), cv2.IMREAD_COLOR)\n",
    "        # opencv frme input // H*W*C(BGR)\n",
    "        # 0-255 3 channel\n",
    "        inputs = torch.Tensor(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))/255 # 여기 Tensor 소문자로 바꾸면 안됨... 차이 알아보기\n",
    "        plt.imshow(inputs)\n",
    "        plt.show()\n",
    "        \n",
    "    def show_tensor(self, t):\n",
    "        plt.imshow(t.numpy().transpose([1,2,0]))\n",
    "        plt.show()\n",
    "        \n",
    "    def plot(self):\n",
    "        self.fit_pca(n_components=4)\n",
    "        _outputs = self.pca.transform(self.outputs.cpu())\n",
    "\n",
    "        plt.figure(figsize=(18,9))\n",
    "        plt.title(\"Vector space, pca_n: \"+str(self.feature_len))\n",
    "        ax = sns.scatterplot(x=0, y=1, hue='name', data=self.df, palette=\"Set1\", legend=\"full\")\n",
    "        ax2 = sns.scatterplot(x=0, y=1, hue='name', data=self.centers, palette=\"Set1\", s=150, legend=None, edgecolor='black')\n",
    "        plt.scatter(_outputs[:,0], _outputs[:,1], marker='x', c='black')\n",
    "        plt.show()\n",
    "\n",
    "    # save plot image for web\n",
    "    def save_plot(self, plotPath):\n",
    "        self.fit_pca(n_components=4)\n",
    "        _outputs = self.pca.transform(self.outputs.cpu())\n",
    "\n",
    "        plt.figure(figsize=(9,9))\n",
    "        plt.title(\"Vector space, pca_n: \"+str(self.feature_len))\n",
    "        ax = sns.scatterplot(x=0, y=1, hue='name', data=self.df, palette=\"Set1\", legend=\"full\", s=30)\n",
    "        ax2 = sns.scatterplot(x=0, y=1, hue='name', data=self.centers, palette=\"Set1\", s=150, legend=None, edgecolor='black')\n",
    "        plt.scatter(_outputs[:,0], _outputs[:,1], marker='x', c='black', s=120)\n",
    "        plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OracleModel(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       "  (roi_upsample): UpsamplingBilinear2d(size=[224, 224], mode=bilinear)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OracleModel()\n",
    "model.to(model.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "차후 util로 리팩토링\n",
    "\"\"\"\n",
    "import PIL\n",
    "from collections import Counter \n",
    "# -------------- 전체 임베딩 피처 디비 생성 --------------\n",
    "def makeAllReferenceCSV(referenceImgDirPath, featuresPath, meanPath, n_sample):\n",
    "    \"\"\"\n",
    "    전체 임베딩 피처 dataframe 생성 후 csv로 저장\n",
    "    \"\"\"\n",
    "    reference_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=referenceImgDirPath,\n",
    "        transform=transforms.Compose([\n",
    "#             transforms.RandomRotation(90, expand=True),\n",
    "            transforms.Resize([224,224]),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomRotation(45),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    )\n",
    "    reference_loader = torch.utils.data.DataLoader(\n",
    "        reference_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # get all data and input to model\n",
    "    _temp = []\n",
    "    for data, target in tqdm(reference_loader):\n",
    "        outputs = model.embed(data.to(model.device)).data # .data 안하면 메모리 오버플로남...\n",
    "        _temp.append(outputs)       \n",
    "\n",
    "    represented = torch.cat(_temp, dim=0)\n",
    "    # raw data label 별 평균 구해두기\n",
    "    df_ref = pd.DataFrame(represented.cpu().numpy())\n",
    "    df_ref['label'] = [reference_dataset.classes[i] for i in reference_dataset.targets*1]\n",
    "    \n",
    "    # sampling\n",
    "    df_ref_sampled = df_ref.groupby('label').apply(pd.DataFrame.sample, n=n_sample).reset_index(drop=True)\n",
    "    reference_means = df_ref_sampled.groupby('label').mean()\n",
    "\n",
    "    # 자료 저장\n",
    "    df_ref_sampled.to_csv(featuresPath, encoding='utf8', index=False)\n",
    "    reference_means.to_csv(meanPath, encoding='utf8', index=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def set_sample_val_img(imgValPath, imgValTempPath, n_class_for_val, musthave_list=[]):\n",
    "    \"\"\"\n",
    "    n개 클래스만 validation 샘플링 하는 함수\n",
    "    @param imgValPath = '.\\\\img_real\\\\val'\n",
    "    @param imgValTempPath = '.\\\\img_real\\\\_val_temp'\n",
    "    @param n_class_for_val = 20\n",
    "    @musthave_list = 반드시 포함할 리스트\n",
    "    \"\"\"\n",
    "    all_dir_list = os.listdir(imgValPath)\n",
    "    sample_dir_list = np.random.permutation(all_dir_list)[:n_class_for_val]\n",
    "    for i in musthave_list:\n",
    "        if i not in sample_dir_list: sample_dir_list = np.concatenate([sample_dir_list, [i]])\n",
    "\n",
    "    # 추출될 이미지 저장 폴더 비우기... 비동기 버그 때문에 sleep넣음 \n",
    "    if os.path.isdir(imgValTempPath): shutil.rmtree(imgValTempPath)\n",
    "    sleep(0.1)\n",
    "    os.makedirs(imgValTempPath)\n",
    "\n",
    "    # copy sampled dir\n",
    "    for class_name in tqdm(sample_dir_list):\n",
    "        if not os.path.isdir(os.path.join(imgValTempPath, class_name)):\n",
    "            os.mkdir(os.path.join(imgValTempPath, class_name))\n",
    "        command = ['cmd', '\\/c', 'copy',  os.path.join(imgValPath, class_name) , os.path.join(imgValTempPath, class_name)]    \n",
    "        subprocess.check_output(command)\n",
    "    sample_dir_list.sort() # 토치 데이터로더도 자동으로 이름순 소팅하므로 꼭 해야함\n",
    "    return list(sample_dir_list)\n",
    "\n",
    "def get_sample_reference(sample_dir_list, featuresPath, meanPath, showData=False):\n",
    "    \"\"\"\n",
    "    n개 클래스 임베딩 & 레이블 & means 추출 \n",
    "    \"\"\"\n",
    "    # load saved embedding features & means\n",
    "    df_ref = pd.read_csv(featuresPath, encoding='utf8')\n",
    "    reference_means = pd.read_csv(meanPath, encoding='utf8', index_col='label')\n",
    "    \n",
    "    if showData:\n",
    "        display(df_ref)\n",
    "        display(reference_means)\n",
    "\n",
    "    df_ref_featere_sampled = None\n",
    "    for c in sample_dir_list:\n",
    "        _df = df_ref[df_ref['label'] == c]\n",
    "        if df_ref_featere_sampled is None: df_ref_featere_sampled = _df\n",
    "        else: df_ref_featere_sampled = df_ref_featere_sampled.append(_df)\n",
    "\n",
    "    df_ref_featere_sampled = df_ref_featere_sampled.reset_index(drop=True)\n",
    "    reference_means_sampled = reference_means.loc[sample_dir_list]\n",
    "    return df_ref_featere_sampled, reference_means_sampled\n",
    "\n",
    "\n",
    "def calcWindowSlidePreds(target_all, preds_all, n_window=7, mode=\"first\"):\n",
    "    \"\"\"\n",
    "    target_all = 정답 리스트\n",
    "    preds_all = 예측값 리스트 \n",
    "    mode = 'all' # first, all\n",
    "    n_window = 3 \n",
    "    @return new_preds_all = []\n",
    "    \"\"\"\n",
    "    \n",
    "    # preds, gt 모아서 데이터프레임화\n",
    "    df = pd.DataFrame(preds_all.numpy())\n",
    "    df['gt']= target_all.numpy()\n",
    "\n",
    "    # 클래스별 인덱스 시작점 계산. 데이터로더 셔플하면 안됨\n",
    "    df_agg = df.groupby('gt').count()\n",
    "    df_agg['end_idx'] = df_agg[0].cumsum()\n",
    "\n",
    "    # 윈도 내 데이터 voting\n",
    "    startidx = 0\n",
    "    new_preds_all = []\n",
    "    for i in df_agg.index:\n",
    "        for idx in range(startidx, df_agg['end_idx'][i]):\n",
    "            bookmarkidx = idx-n_window+1 if idx-n_window>=0 else 0\n",
    "            if idx-n_window<startidx:\n",
    "                bookmarkidx = startidx\n",
    "\n",
    "            if mode=='all': # 1~3위 까지 전체 사용 모드, [::-1] 은 가장 최근 값을 맨 앞으로 보내려고\n",
    "                res = np.concatenate(preds_all[bookmarkidx:idx+1].numpy()[::-1])\n",
    "            elif mode=='first': # 1위만 사용모드       \n",
    "                res = preds_all[bookmarkidx:idx+1, 0].numpy()[::-1]\n",
    "            res_cnt = Counter(res) # counter는 먼저나온 값이 먼저 등록됨, 즉 카운터가 동률이면 최신값이 가장 먼저 나옴\n",
    "#             print(idx-n_window, idx, bookmarkidx, idx+1, res, res_cnt.most_common()[:2], '[Pred]:', res_cnt.most_common()[0][0])\n",
    "            new_preds_all.append(res_cnt.most_common()[0][0])\n",
    "#         print('-------------------------')\n",
    "        startidx = df_agg['end_idx'][i]\n",
    "    return new_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- non max suppression -------------\n",
    "# https://wns349.github.io/2018/10/16/nms/\n",
    "from itertools import combinations, permutations\n",
    "# Calc IOU\n",
    "def calcIOU(box1, box2):\n",
    "    \"\"\"\n",
    "    box - x,y,w,h\n",
    "    \"\"\"\n",
    "    area_box1 = box1[2]*box1[3]\n",
    "    area_box2 = box2[2]*box2[3]\n",
    "    x1_max = max(box1[0], box2[0])\n",
    "    x2_min = min(box1[0]+box1[2], box2[0]+box2[2])\n",
    "    y1_max = max(box1[1], box2[1])\n",
    "    y2_min = min(box1[1]+box1[3], box2[1]+box2[3])\n",
    "    \n",
    "    area_intersection = max(0, x2_min-x1_max) * max(0, y2_min-y1_max)\n",
    "    area_union = area_box1+area_box2-area_intersection +1e-9\n",
    "    return area_intersection/area_union\n",
    "\n",
    "def non_max_sup_one_class(bboxes, threshold=0.2, descending=False):\n",
    "    \"\"\"\n",
    "    @params threshold - \n",
    "    @params ascending - 기본이 내림차순,\n",
    "    \"\"\"\n",
    "    bboxes = list(bboxes)\n",
    "    bboxes.sort(key = lambda x: x[2], reverse=descending) # 거리값이므로 오름차순, 확률이면 내림차순  \n",
    "    bboxes = np.array(bboxes)\n",
    "    keeps = [True]*len(bboxes)\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        if not keeps[i]: continue\n",
    "        for j in range(i+1, len(bboxes)):\n",
    "            if not keeps[i]: continue\n",
    "            iou_res = calcIOU(bbox[0], bboxes[j][0])\n",
    "            if iou_res>threshold: keeps[j] = False\n",
    "    return bboxes[keeps]\n",
    "\n",
    "def rpn3(box, n_slice_x, n_slice_y):\n",
    "    \"\"\"\n",
    "    n분할 rpn 조합\n",
    "    @params box: x,y,w,h\n",
    "    @params im: nomalized image tensor N x C x W x H\n",
    "    @return \n",
    "    \"\"\"\n",
    "    w = box[2]/n_slice_x\n",
    "    h = box[3]/n_slice_y\n",
    "    \n",
    "    rois = []\n",
    "    boxes = []\n",
    "    \n",
    "    coords = np.array(np.meshgrid(list(range(n_slice_x+1)), list(range(n_slice_y+1)))).T.reshape(-1,2)\n",
    "    \n",
    "    for a,b in combinations(coords,2):\n",
    "        if a[0]>=b[0] or a[1]>= b[1]: continue # 넓이 없는 사각형 제거\n",
    "#         print(a,b)\n",
    "        boxes.append([box[0]+a[0]*w, box[1]+a[1]*h, \n",
    "                      (b[0]-a[0])*w, (b[1]-a[1])*h]) # x,y,w,h\n",
    "\n",
    "    boxes = torch.tensor(boxes)\n",
    "    return boxes\n",
    "\n",
    "def get_rois(images, featuremaps, bboxes):\n",
    "    \"\"\"\n",
    "    roi-align from feature map\n",
    "    @params images - 원본 이미지 (사이즈 계산용)\n",
    "    @params featuremaps - CNN Backbone 거쳐 나온 것\n",
    "    @params bboxes - 원본 이미지에서의 bboxes (x,y,w,h)\n",
    "    @return 피처맵에서 스케일된 bbox부분 7x7로 roi-align된 피처맵\n",
    "    \"\"\"\n",
    "    # calc bbox ratio\n",
    "    ratio_y = featuremaps.shape[2]/images.shape[2]\n",
    "    ratio_x = featuremaps.shape[3]/images.shape[3]\n",
    "    bboxes_scaled = bboxes.clone()#.detach()\n",
    "\n",
    "    bboxes_scaled[:,0] = bboxes_scaled[:,0]*ratio_x # for x\n",
    "    bboxes_scaled[:,1] = bboxes_scaled[:,1]*ratio_y # for y\n",
    "    bboxes_scaled[:,2] = bboxes_scaled[:,2]*ratio_x # for w\n",
    "    bboxes_scaled[:,3] = bboxes_scaled[:,3]*ratio_y # for h\n",
    "\n",
    "    # x,y,w,h -> x1, y1, x2, y2 그래디언트 학습되는 변수가 아니므로 inplace 계산 들어가도 괜찮다\n",
    "    bboxes_scaled[:, 2] = bboxes_scaled[:, 0] + bboxes_scaled[:, 2]\n",
    "    bboxes_scaled[:, 3] = bboxes_scaled[:, 1] + bboxes_scaled[:, 3]\n",
    "    \n",
    "    crops = torchvision.ops.roi_align(featuremaps, [bboxes_scaled], [7,7])\n",
    "    return crops\n",
    "\n",
    "# -------------------- edgebox RPN ----------------------------\n",
    "# https://github.com/opencv/opencv_contrib/blob/master/modules/ximgproc/samples/edgeboxes_demo.py\n",
    "# https://donghwa-kim.github.io/EdgeBoxes.html - 엣지박스 간단 설명\n",
    "\n",
    "edge_model = \".\\\\server\\\\vu-visor\\\\model.yml.gz\"\n",
    "edge_detection = cv2.ximgproc.createStructuredEdgeDetection(edge_model)\n",
    "\n",
    "def rpn(im_opencv, num_boxs, scale=1, min_score=0.01):\n",
    "    \"\"\"\n",
    "    region proposal network\n",
    "    \"\"\"\n",
    "    global edge_detection\n",
    "    \n",
    "    def makeEdgeBox(scale):\n",
    "        im_opencv_scaled = cv2.resize(im_opencv, (int(im_opencv.shape[1]*scale), int(im_opencv.shape[0]*scale)), \n",
    "                                      interpolation=cv2.INTER_CUBIC).astype(np.float32)\n",
    "\n",
    "        edges = edge_detection.detectEdges(im_opencv_scaled / 255.0)\n",
    "        orimap = edge_detection.computeOrientation(edges)\n",
    "        edges = edge_detection.edgesNms(edges, orimap)\n",
    "        \n",
    "        edge_boxes = cv2.ximgproc.createEdgeBoxes() \n",
    "        edge_boxes.setBeta(0.75) # beta=0.1, nms threshold for object proposals.\n",
    "        edge_boxes.setMaxBoxes(num_boxs)\n",
    "        edge_boxes.setMinScore(min_score) # box score\n",
    "        boxes, scores = edge_boxes.getBoundingBoxes(edges, orimap)\n",
    "        return boxes, scores, im_opencv_scaled\n",
    "    \n",
    "    (boxes, scores, im_opencv_scaled) = makeEdgeBox(scale)\n",
    "    # bbox 하나도 없으면 전체샷이라도 저장\n",
    "    if len(boxes)==0: \n",
    "        boxes=np.array([[0,0,im_opencv_scaled.shape[1],im_opencv_scaled.shape[0]]])\n",
    "        scores=np.array([[min_score]])\n",
    "    boxes = (boxes/scale).round().astype(np.int)\n",
    "    # 박스 개수 절반보다 모자라면 스케일 키워서 한번더\n",
    "    if len(boxes)<(num_boxs/2):\n",
    "        scale = scale*2\n",
    "        (_boxes, _scores, im_opencv_scaled) = makeEdgeBox(scale)\n",
    "        if len(_boxes)>0:\n",
    "            _boxes = (_boxes/scale).round().astype(np.int)\n",
    "            boxes = np.concatenate([boxes,_boxes])[:num_boxs] # 이전 스케일의 박스와 concat\n",
    "            scores = np.concatenate([scores,_scores])[:num_boxs]\n",
    "\n",
    "    return boxes, scores\n",
    "\n",
    "\n",
    "# im = cv2.imread(\".\\\\server/vu-visor/predict.jpg\")\n",
    "# %time boxes, scores = rpn(im, num_boxs=10, scale=1, min_score=0.01)\n",
    "# for idx, (b, s) in enumerate(zip(boxes, scores)):\n",
    "#     x, y, w, h = b\n",
    "#     cv2.rectangle(im, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)\n",
    "#     cv2.putText(im, str(s), (x+3, y+17), cv2.FONT_HERSHEY_SIMPLEX, .6, (0,0,225), 2)                             \n",
    "\n",
    "# # # cv2.imshow(\"edges\", edges);\n",
    "# cv2.imshow(\"edgeboxes\", im)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()  \n",
    "\n",
    "def calc_tp_state(boxes_ans, boxes_model, confidiences_model, filename=None):\n",
    "    \"\"\"\n",
    "    average precision 계산용 데이터 프레임 리턴\n",
    "    @params boxes - x,y,w,h\n",
    "    state_array = [[filename, conf, state, iou], [filename, conf, state, iou], ...]\n",
    "    \"\"\"\n",
    "    threshold_IOU = 0.5 # 이걸 넘어야 TP, 못넘으면 FP\n",
    "    state_array = []\n",
    "    for model_box_idx, model_box in enumerate(boxes_model):\n",
    "        state = [filename, confidiences_model[model_box_idx], 0, 0] # default가 FP\n",
    "        for ans_box_idx, ans_box in enumerate(boxes_ans):\n",
    "            _IOU = float(calcIOU(model_box, ans_box))\n",
    "            if _IOU>threshold_IOU: # IOU넘는게 하나라도 있으면 TP인 경우\n",
    "                state = [filename, confidiences_model[model_box_idx], 1, _IOU] # default가 FP\n",
    "                break\n",
    "            if _IOU>state[3]: state[3]=_IOU # FP경우에도 최고 IOU기록\n",
    "        state_array.append(state)\n",
    "    _df = pd.DataFrame(state_array, columns=['image', 'confidience', 'TP', 'IOU'])\n",
    "    return _df\n",
    "\n",
    "from collections import Counter\n",
    "# https://github.com/rafaelpadilla/Object-Detection-Metrics\n",
    "\n",
    "def calc_AP(dataset_classes, label, target_all, dfs):\n",
    "    target = dataset_classes.index(label)\n",
    "    n_gt = Counter(target_all)\n",
    "\n",
    "    _df = dfs[label].sort_values(by='confidience', ascending=False) # sort by conf\n",
    "\n",
    "    _df['FP'] = [0 if tp==1 else 1 for tp in _df['TP']]\n",
    "    _df['cum_TP'] = _df['TP'].cumsum()\n",
    "    _df['cum_FP'] = _df['FP'].cumsum()\n",
    "    _df['precision'] = [tp/(tp+fp) for tp,fp in zip(_df['cum_TP'],_df['cum_FP'])]\n",
    "    _df['recall'] = _df['cum_TP']/n_gt[target]\n",
    "    _df['precision_interpolated'] = [_df['precision'][i:].max() for i in range(len(_df['recall']))] \n",
    "    _df.reset_index(inplace=True)\n",
    "\n",
    "    precisions = [_df['precision_interpolated'][_df['recall'].between(0.1*(i-1),0.1*i,inclusive=True)].min() for i in range(11)]\n",
    "    precisions[0] = 1\n",
    "    precisions = np.array([0 if p is np.nan else p for p in precisions])\n",
    "    AP = precisions.mean()\n",
    "#     print('[class]:', label, ', [AP]:', AP)\n",
    "#     print('[Precisions]', precisions)\n",
    "\n",
    "#     plt.plot(_df['recall'], _df['precision'])\n",
    "#     plt.plot(_df['recall'], _df['precision_interpolated'], color='r')\n",
    "#     plt.grid()\n",
    "#     plt.xlim([0,1]), plt.title(label+' 11 point p-r curve')\n",
    "#     plt.xlabel('recall'), plt.ylabel('precision')\n",
    "#     plt.show()\n",
    "\n",
    "    # display(_df)\n",
    "    return AP, _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doClassification():\n",
    "    # ---------------------------- validation set 로드 ---------------------------------\n",
    "\n",
    "    val_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=imgValPath,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # -------------- 레퍼런스 데이터에서 n개 클래스 임베딩 & 레이블 & means 추출 -----------------\n",
    "    featuresPath = 'cosmetic_features_cos.csv'\n",
    "    meanPath = 'cosmetic_means_cos.csv'\n",
    "\n",
    "    sample_dir_list = val_dataset.classes\n",
    "    df_ref_featere_sampled, reference_means_sampled = get_sample_reference(sample_dir_list, featuresPath, meanPath, showData=False)\n",
    "\n",
    "    # ---------------------------- 레퍼런스셋 로드 및 임베딩 세팅 ---------------------------------\n",
    "    model.setReferenceDataset(val_dataset.classes, df_ref_featere_sampled, reference_means_sampled)\n",
    "\n",
    "    # ---------------------------- validation set 정확도 측정 ---------------------------------\n",
    "    print('**********classification***********')\n",
    "\n",
    "    target_all = []\n",
    "    preds_all = []\n",
    "    for data, target in tqdm(val_loader):\n",
    "        data = data.to(model.device)\n",
    "        preds, preds_dist = model.inference_tensor3(model(data).data, 'cos', knn=USE_KNN)\n",
    "        target_all.append(target)\n",
    "        preds_all.append(preds)\n",
    "        \n",
    "\n",
    "    target_all=torch.cat(target_all, dim=0)\n",
    "    preds_all=torch.cat(preds_all, dim=0).data.cpu()\n",
    "\n",
    "    return target_all, preds_all\n",
    "\n",
    "def doBboxClassification():\n",
    "    global detect_ref_dir, class_path\n",
    "    print('********** [n_sample]:',n_sample,'[ref]:',ref,'[val]:',val,'[knn]:',USE_KNN,'[texture]:',USE_TEXTURE,'[threshold]:',model.threshold,'***********')\n",
    "\n",
    "    detect_acc_dataset = MyDataset(detect_ref_dir, class_path, \n",
    "                          transform=transforms.Compose([\n",
    "    #         transforms.Resize([224,224]),\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "    #         transforms.RandomRotation(45),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ---------------------------- val set 정확도 측정 ---------------------------------\n",
    "    print('**********classification-bbox***********')\n",
    "    target_all = []\n",
    "    preds_all = []\n",
    "#     for i in tqdm(np.random.permutation(len(detect_acc_dataset))):\n",
    "    for i in tqdm(range(len(detect_acc_dataset))):\n",
    "        im_tensor, targets_gt, boxes_gt = detect_acc_dataset[i]\n",
    "        featuremaps = model(im_tensor.to(model.device))\n",
    "        target_all = np.concatenate([target_all,  targets_gt])\n",
    "        \n",
    "        _boxes_cuda = torch.from_numpy(boxes_gt).float().cuda()\n",
    "        rois = get_rois(im_tensor, featuremaps, _boxes_cuda)\n",
    "        if(USE_TEXTURE): preds = model.inference_tensor_texture(rois, 'cos', knn=USE_KNN, SUM=True)\n",
    "        else: preds, preds_dist= model.inference_tensor3(rois, 'cos', knn=USE_KNN)\n",
    "        preds_all.append(preds.data)\n",
    "\n",
    "    target_all=torch.tensor(target_all).data.int()\n",
    "    preds_all=torch.cat(preds_all, dim=0).data.int().cpu()\n",
    "    \n",
    "    # groupby 통계량 보기\n",
    "    _res = []\n",
    "    for t,p in zip(target_all.numpy(), preds_all.numpy()):\n",
    "        _res.append([detect_acc_dataset.classes[t],t,p,t==p[0], t in p])\n",
    "    _df_res = pd.DataFrame(_res, columns=['label', 'target', 'pred', 'top1', 'top3'])\n",
    "    res_agg_df = _df_res.groupby('label').mean()\n",
    "\n",
    "    return target_all, preds_all, res_agg_df['top1'].as_matrix()\n",
    "\n",
    "def doDetection():\n",
    "    global detect_ref_dir, class_path\n",
    "    print('**********detection***********')\n",
    "    # ---------------------------- val set 로드 ---------------------------------\n",
    "    voc_val_dataset = MyDataset(detect_ref_dir, class_path, \n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "\n",
    "    voc_transform = transforms.Compose([\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    # assert val_dataset.classes==sample_dir_list, \"샘플링 클래스와 데이터로더 클래스가 다름\"\n",
    "    # ---------------------------- val set 정확도 측정 ---------------------------------\n",
    "\n",
    "    fps=0\n",
    "    # model.threshold = 0.9*0.08 #0.85 #0.4\n",
    "    SHOW_IMAGE = False # 이미지 볼지 말지\n",
    "    dfs = {c:pd.DataFrame(columns=['image', 'confidience', 'TP', 'IOU']) for c in voc_val_dataset.classes} # average precision 저장\n",
    "    target_all = [] #ground truth bboxs\n",
    "\n",
    "    # print('[num_class]:', len(sample_dir_list), sample_dir_list)\n",
    "#     for i in tqdm(np.random.permutation(len(voc_val_dataset))):\n",
    "    for i in tqdm(range(len(voc_val_dataset))):\n",
    "        startTime = time.time()\n",
    "        data, targets_gt, boxes_gt = voc_val_dataset[i] # 정답 데이터, 레이블, bbox\n",
    "\n",
    "        frame = data.clone().detach().mul(255).squeeze().numpy().astype(np.uint8).transpose([1,2,0])\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # opencv image need to convert BGR -> RGB\n",
    "\n",
    "        im_tensor = voc_transform(data.squeeze()).to(model.device).data.unsqueeze(0)\n",
    "        featuremaps = model(im_tensor)\n",
    "        target_all = np.concatenate([target_all,  targets_gt])\n",
    "            \n",
    "        # region proposal network extracts ROIs\n",
    "        boxes, scores = rpn(frame, num_boxs=n_rpn_box, scale=1, min_score=0.01)\n",
    "        cv2.putText(frame, str(len(boxes)), (data.shape[-1]-80,30), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,100,225), thickness=2) # rpn생성 box 개수\n",
    "        \n",
    "        if SHOW_IMAGE:    \n",
    "            # render ground truth roi prediction\n",
    "            cv2.putText(frame, str(float(preds_dist.mean()))[:5], (20,30), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,200,225), thickness=2) # roi평균 유사도\n",
    "            _boxes_gt_cuda = torch.from_numpy(boxes_gt).float().cuda()\n",
    "            rois_gt = get_rois(im_tensor, featuremaps, _boxes_gt_cuda)\n",
    "            preds_gt, pred_dists_gt = model.inference_tensor3(rois_gt, 'cos', knn=USE_KNN)    \n",
    "            for pred, dist, b in zip(preds_gt[:,0], pred_dists_gt[:,0], boxes_gt):\n",
    "                pred_label = voc_val_dataset.classes[pred]\n",
    "                res_text = \"(\"+str(float(dist))[:5]+\")\"+pred_label\n",
    "                x, y, w, h = b\n",
    "                cv2.putText(frame, res_text, (x+3, y+12), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.5, color=(255,0,255), thickness=1)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)  \n",
    "            \n",
    "        # inference scores, boxes\n",
    "        _boxes_cuda = torch.from_numpy(boxes).float().cuda()\n",
    "        rois = get_rois(im_tensor, featuremaps, _boxes_cuda)\n",
    "        preds, preds_dist= model.inference_tensor3(rois, 'cos', knn=USE_KNN)\n",
    "        \n",
    "    #     # make new scores\n",
    "    #     scores = torch.from_numpy(scores).to(model.device)\n",
    "    #     preds_dist = preds_dist*scores\n",
    "        \n",
    "        # objectness filterling\n",
    "        filter_idx = (preds_dist[:,0]>model.threshold).type(torch.bool).cpu()\n",
    "        if any(filter_idx): # 필터 통과하는거 하나라도 있어야 함\n",
    "            _boxes_cuda = _boxes_cuda[filter_idx]\n",
    "            preds = preds[filter_idx]\n",
    "            preds_dist = preds_dist[filter_idx]\n",
    "            rois = rois[filter_idx]\n",
    "            \n",
    "            for i in range(1):\n",
    "                boxes_cuda = _boxes_cuda.clone().detach()\n",
    "                if i==1:\n",
    "                    # UBBR adjusted bboxes\n",
    "                    offsets = UBBR_model.fc(rois) \n",
    "            #                 offsets = UBBR_model(im_tensor, offsets)        \n",
    "                    # reg 적용한 random boxes\n",
    "                    boxes_cuda = regression_transform(boxes_cuda, offsets)\n",
    "\n",
    "                # non-maximum-suppression\n",
    "                bboxes_all = np.array(list(zip(boxes_cuda.cpu(), preds.cpu().numpy()[:,0], preds_dist.cpu().numpy()[:,0])), dtype=np.object)\n",
    "                bboxes_all_nms = []\n",
    "                for cls in set(bboxes_all[:,1]):\n",
    "                    bboxes_all_nms.append(non_max_sup_one_class(bboxes_all[bboxes_all[:,1]==cls], threshold=0.05, descending=model.sort_order_descending))\n",
    "                bboxes_all_nms = np.concatenate(bboxes_all_nms)\n",
    "\n",
    "                # render frame\n",
    "                if SHOW_IMAGE:\n",
    "                    for idx, (box, pred, dist) in enumerate(bboxes_all_nms):\n",
    "                        pred_label = model.reference_classes[pred]\n",
    "                        res_text = \"(\"+str(float(dist))[:5]+\")\"+pred_label\n",
    "                        x, y, w, h = box\n",
    "                        if i==1: cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 1, cv2.LINE_AA) # UBBR\n",
    "                        else: cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 1, cv2.LINE_AA)\n",
    "                        cv2.putText(frame, res_text, (x+3, y+12), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,225), 1)        \n",
    "\n",
    "            # collect statistics for mAP\n",
    "            # ans data - targets_gt, boxes_gt, filename_gt\n",
    "            for t in set(bboxes_all_nms[:,1]): # iterate all class\n",
    "                bboxes_all_nms_per_class = bboxes_all_nms[bboxes_all_nms[:,1]==t]\n",
    "                boxes_gt_per_class = boxes_gt[targets_gt==t]\n",
    "\n",
    "                boxes_model = bboxes_all_nms_per_class[:,0]\n",
    "                confidiences_model = bboxes_all_nms_per_class[:,2]\n",
    "                _df = calc_tp_state(boxes_gt_per_class, boxes_model, confidiences_model, filename='filename_gt') # class 하나만 가정, \n",
    "                _class = voc_val_dataset.classes[t]\n",
    "                dfs[_class] = dfs[_class].append(_df, ignore_index=True) # update df \n",
    "        \n",
    "        # only render UI\n",
    "        if SHOW_IMAGE:\n",
    "            # frame = cv2.resize(frame, (int(frame.shape[1]*1.5), int(frame.shape[0]*1.5)), interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow('frame',frame)\n",
    "            k = cv2.waitKey(0) & 0xff \n",
    "            if k == 27: # esc\n",
    "                break\n",
    "\n",
    "        endTime = time.time()\n",
    "        fps = int(1/(endTime - startTime))\n",
    "            \n",
    "    cv2.destroyAllWindows()        \n",
    "\n",
    "    # calc AP, mAP, TPs, FPs\n",
    "    AP_list = []\n",
    "    df_res_list = {}\n",
    "    for cls in voc_val_dataset.classes:\n",
    "        AP, df_res = calc_AP(voc_val_dataset.classes, cls, target_all, dfs)\n",
    "        AP_list.append(AP)\n",
    "        df_res_list[cls] = df_res\n",
    "    AP_list = np.array(AP_list)\n",
    "\n",
    "    mAP = AP_list.mean()\n",
    "\n",
    "    tps = []\n",
    "    fps = []\n",
    "    for k in df_res_list:\n",
    "        try:\n",
    "            tps.append(list(df_res_list[k]['cum_TP'])[-1])\n",
    "            fps.append(list(df_res_list[k]['cum_FP'])[-1])\n",
    "        except:\n",
    "            tps.append(np.nan)\n",
    "            fps.append(np.nan)\n",
    "\n",
    "    return mAP, AP_list, fps, tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allpath = {\n",
    "  'refPath':{\n",
    "    # 이건 floor, frontview, \n",
    "    'trimed-frontview':'.\\\\new-cosmetic-frontview\\\\images_bbox',\n",
    "    'trimed-floor':'.\\\\new-cosmetic-floor\\\\images_bbox',\n",
    "    'trimed-office':'.\\\\new-cosmetic-office\\\\images_bbox',\n",
    "    'original-frontview':'.\\\\new-cosmetic-frontview\\\\images_ext',\n",
    "    'original-floor':'.\\\\new-cosmetic-floor\\\\images_ext',\n",
    "    'original-office':'.\\\\new-cosmetic-office\\\\images_ext',\n",
    "\n",
    "    'original': '.\\\\references\\\\original',\n",
    "    'full': '.\\\\references\\\\full',\n",
    "    'no-masked-trimed': '.\\\\references\\\\images_trimed',\n",
    "    'no-masked-trimed-10shot':'C:\\\\Users\\\\LSW\\\\Downloads\\\\mask_img\\\\images_trimed_10shot_class',\n",
    "    'masked-trimed':'C:\\\\Users\\\\LSW\\\\Downloads\\\\mask_img\\\\masked_images_trimed_class',\n",
    "  },\n",
    "  'clsValPath':{\n",
    "    #   for acc-full\n",
    "    'frontview2':'.\\\\new-cosmetic-frontview2\\\\images_full',\n",
    "    'wall':'.\\\\new-cosmetic-wall\\\\images_full',\n",
    "    'floor2':'.\\\\new-cosmetic-floor2\\\\images_full',\n",
    "    # 'office':'.\\\\new-cosmetic-office\\\\images_full',\n",
    "  },\n",
    "  'detectionValPath':{\n",
    "    #   for acc-bbox, mAP\n",
    "    'frontview2':'C:\\\\Users\\\\LSW\\\\Desktop\\\\Smart_mirror\\\\Yolo_mark\\\\x64\\\\Release\\\\cosmetic-data\\\\new-cosmetic-frontview2\\\\temp',\n",
    "    'wall':'C:\\\\Users\\\\LSW\\\\Desktop\\\\Smart_mirror\\\\Yolo_mark\\\\x64\\\\Release\\\\cosmetic-data\\\\new-cosmetic-wall\\\\temp',\n",
    "    'floor2':'C:\\\\Users\\\\LSW\\\\Desktop\\\\Smart_mirror\\\\Yolo_mark\\\\x64\\\\Release\\\\cosmetic-data\\\\new-cosmetic-floor2\\\\temp',\n",
    "    # 'office':'C:\\\\Users\\\\LSW\\\\Desktop\\\\Smart_mirror\\\\Yolo_mark\\\\x64\\\\Release\\\\cosmetic-data\\\\new-cosmetic-office\\\\temp',\n",
    "  },\n",
    "  'classPath':'C:\\\\Users\\\\LSW\\\\Desktop\\\\Smart_mirror\\\\Yolo_mark\\\\x64\\\\Release\\\\cosmetic-data\\\\obj.names'\n",
    "}\n",
    "\n",
    "cols = ['timestamp', 'ref', 'val', 'n-sample', 'knn', 'texture', 'slide', 'threshold', 'n-box', 'acc-full', 'acc-bbox', 'mAP', 'memo',\n",
    "        *['AP'+str(c) for c in range(31)],\n",
    "        *['FP'+str(c) for c in range(31)],\n",
    "        *['TP'+str(c) for c in range(31)],\n",
    "        *['ACC'+str(c) for c in range(31)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 .\\new-cosmetic-frontview\\images_bbox\\AHC-AGELESS REAL EYE CREAM FOR FACE\n",
      "15 .\\new-cosmetic-frontview\\images_bbox\\AHC-Aura Secret Toneup Cream\n",
      "20 .\\new-cosmetic-frontview\\images_bbox\\AHC-ONLY FOR MAN LOTION\n",
      "13 .\\new-cosmetic-frontview\\images_bbox\\APIEU-데카소사이드 시카 겔 데이크림\n",
      "16 .\\new-cosmetic-frontview\\images_bbox\\APIEU-스타트업 포어 프라이머\n",
      "14 .\\new-cosmetic-frontview\\images_bbox\\BANILACO-프라임 프라이머 클래식\n",
      "16 .\\new-cosmetic-frontview\\images_bbox\\BELIF-The true cream AQUA BOMB\n",
      "18 .\\new-cosmetic-frontview\\images_bbox\\CLIO-스파클링 라인 프리즘 에어 아이섀도우\n",
      "15 .\\new-cosmetic-frontview\\images_bbox\\FERRAGAMO-INCANTO CHARMS\n",
      "20 .\\new-cosmetic-frontview\\images_bbox\\FERRARI-SCUDERIA BLACK EAU DE TOILETTE SPRAY\n",
      "12 .\\new-cosmetic-frontview\\images_bbox\\FROMNATURE-에이지 인텐스 트리트먼트 에센스\n",
      "13 .\\new-cosmetic-frontview\\images_bbox\\GENABELLE-LASOR SOOTHING SUNSCREEN\n",
      "14 .\\new-cosmetic-frontview\\images_bbox\\GREEN FINGER-KIDS FACIAL LOTION\n",
      "14 .\\new-cosmetic-frontview\\images_bbox\\HERA-BLACK CUSHION\n",
      "16 .\\new-cosmetic-frontview\\images_bbox\\HOLIKA HOLIKA-GOOD CERA SUPER CERAMIDE MIST\n",
      "17 .\\new-cosmetic-frontview\\images_bbox\\ILLIYOON-세라마이드 아토 수딩 젤\n",
      "15 .\\new-cosmetic-frontview\\images_bbox\\INNISFREE-ALOE REVITAL SOOTHING GEL\n",
      "12 .\\new-cosmetic-frontview\\images_bbox\\INNISFREE-GREEN TEA SEED ESSENC IN LOTION\n",
      "16 .\\new-cosmetic-frontview\\images_bbox\\INNISFREE-JEJU CHERRY BLOSSOM SKIN\n",
      "11 .\\new-cosmetic-frontview\\images_bbox\\INNISFREE-NO SEBUM Mineral Powder\n",
      "16 .\\new-cosmetic-frontview\\images_bbox\\INNISFREE-마이 블러셔 햇살 가득 장미\n",
      "12 .\\new-cosmetic-frontview\\images_bbox\\ISOI-인텐시브 에너자이징크림\n",
      "12 .\\new-cosmetic-frontview\\images_bbox\\MACQUEEN-쥬얼포텐 아이글리터\n",
      "14 .\\new-cosmetic-frontview\\images_bbox\\MISSHA-레이어 블러링 모공커버 프라이머\n",
      "14 .\\new-cosmetic-frontview\\images_bbox\\NEUTROGENA-HAND CREAM\n",
      "12 .\\new-cosmetic-frontview\\images_bbox\\PERIPERA-핑크의순간 컬렉션 잉크 컬러 마스카라\n",
      "15 .\\new-cosmetic-frontview\\images_bbox\\RIRE-BUBLE BUBLE LIP MASK\n",
      "13 .\\new-cosmetic-frontview\\images_bbox\\ROKKISS-카렌듈라 에멀젼\n",
      "13 .\\new-cosmetic-frontview\\images_bbox\\SENKA-PERFECT WHIP\n",
      "13 .\\new-cosmetic-frontview\\images_bbox\\TONYMOLY-피키비키 아트 팝 코렉팅 베이스\n",
      "15 .\\new-cosmetic-frontview\\images_bbox\\VICHY-오 떼르말 미네랄 온천수 미스트\n",
      "15 .\\new-cosmetic-floor\\images_bbox\\AHC-AGELESS REAL EYE CREAM FOR FACE\n",
      "36 .\\new-cosmetic-floor\\images_bbox\\AHC-Aura Secret Toneup Cream\n",
      "29 .\\new-cosmetic-floor\\images_bbox\\AHC-ONLY FOR MAN LOTION\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\APIEU-데카소사이드 시카 겔 데이크림\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\APIEU-스타트업 포어 프라이머\n",
      "28 .\\new-cosmetic-floor\\images_bbox\\BANILACO-프라임 프라이머 클래식\n",
      "17 .\\new-cosmetic-floor\\images_bbox\\BELIF-The true cream AQUA BOMB\n",
      "19 .\\new-cosmetic-floor\\images_bbox\\CLIO-스파클링 라인 프리즘 에어 아이섀도우\n",
      "17 .\\new-cosmetic-floor\\images_bbox\\FERRAGAMO-INCANTO CHARMS\n",
      "21 .\\new-cosmetic-floor\\images_bbox\\FERRARI-SCUDERIA BLACK EAU DE TOILETTE SPRAY\n",
      "15 .\\new-cosmetic-floor\\images_bbox\\FROMNATURE-에이지 인텐스 트리트먼트 에센스\n",
      "20 .\\new-cosmetic-floor\\images_bbox\\GENABELLE-LASOR SOOTHING SUNSCREEN\n",
      "27 .\\new-cosmetic-floor\\images_bbox\\GREEN FINGER-KIDS FACIAL LOTION\n",
      "28 .\\new-cosmetic-floor\\images_bbox\\HERA-BLACK CUSHION\n",
      "42 .\\new-cosmetic-floor\\images_bbox\\HOLIKA HOLIKA-GOOD CERA SUPER CERAMIDE MIST\n",
      "19 .\\new-cosmetic-floor\\images_bbox\\ILLIYOON-세라마이드 아토 수딩 젤\n",
      "33 .\\new-cosmetic-floor\\images_bbox\\INNISFREE-ALOE REVITAL SOOTHING GEL\n",
      "24 .\\new-cosmetic-floor\\images_bbox\\INNISFREE-GREEN TEA SEED ESSENC IN LOTION\n",
      "30 .\\new-cosmetic-floor\\images_bbox\\INNISFREE-JEJU CHERRY BLOSSOM SKIN\n",
      "21 .\\new-cosmetic-floor\\images_bbox\\INNISFREE-NO SEBUM Mineral Powder\n",
      "25 .\\new-cosmetic-floor\\images_bbox\\INNISFREE-마이 블러셔 햇살 가득 장미\n",
      "18 .\\new-cosmetic-floor\\images_bbox\\ISOI-인텐시브 에너자이징크림\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\MACQUEEN-쥬얼포텐 아이글리터\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\MISSHA-레이어 블러링 모공커버 프라이머\n",
      "22 .\\new-cosmetic-floor\\images_bbox\\NEUTROGENA-HAND CREAM\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\PERIPERA-핑크의순간 컬렉션 잉크 컬러 마스카라\n",
      "16 .\\new-cosmetic-floor\\images_bbox\\RIRE-BUBLE BUBLE LIP MASK\n",
      "17 .\\new-cosmetic-floor\\images_bbox\\ROKKISS-카렌듈라 에멀젼\n",
      "28 .\\new-cosmetic-floor\\images_bbox\\SENKA-PERFECT WHIP\n",
      "19 .\\new-cosmetic-floor\\images_bbox\\TONYMOLY-피키비키 아트 팝 코렉팅 베이스\n",
      "17 .\\new-cosmetic-floor\\images_bbox\\VICHY-오 떼르말 미네랄 온천수 미스트\n",
      "42 .\\new-cosmetic-frontview\\images_ext\\AHC-AGELESS REAL EYE CREAM FOR FACE\n",
      "54 .\\new-cosmetic-frontview\\images_ext\\AHC-Aura Secret Toneup Cream\n",
      "70 .\\new-cosmetic-frontview\\images_ext\\AHC-ONLY FOR MAN LOTION\n",
      "50 .\\new-cosmetic-frontview\\images_ext\\APIEU-데카소사이드 시카 겔 데이크림\n",
      "54 .\\new-cosmetic-frontview\\images_ext\\APIEU-스타트업 포어 프라이머\n",
      "49 .\\new-cosmetic-frontview\\images_ext\\BANILACO-프라임 프라이머 클래식\n",
      "53 .\\new-cosmetic-frontview\\images_ext\\BELIF-The true cream AQUA BOMB\n",
      "53 .\\new-cosmetic-frontview\\images_ext\\CLIO-스파클링 라인 프리즘 에어 아이섀도우\n",
      "53 .\\new-cosmetic-frontview\\images_ext\\FERRAGAMO-INCANTO CHARMS\n",
      "72 .\\new-cosmetic-frontview\\images_ext\\FERRARI-SCUDERIA BLACK EAU DE TOILETTE SPRAY\n",
      "39 .\\new-cosmetic-frontview\\images_ext\\FROMNATURE-에이지 인텐스 트리트먼트 에센스\n",
      "45 .\\new-cosmetic-frontview\\images_ext\\GENABELLE-LASOR SOOTHING SUNSCREEN\n",
      "50 .\\new-cosmetic-frontview\\images_ext\\GREEN FINGER-KIDS FACIAL LOTION\n",
      "50 .\\new-cosmetic-frontview\\images_ext\\HERA-BLACK CUSHION\n",
      "52 .\\new-cosmetic-frontview\\images_ext\\HOLIKA HOLIKA-GOOD CERA SUPER CERAMIDE MIST\n",
      "59 .\\new-cosmetic-frontview\\images_ext\\ILLIYOON-세라마이드 아토 수딩 젤\n",
      "53 .\\new-cosmetic-frontview\\images_ext\\INNISFREE-ALOE REVITAL SOOTHING GEL\n",
      "42 .\\new-cosmetic-frontview\\images_ext\\INNISFREE-GREEN TEA SEED ESSENC IN LOTION\n",
      "60 .\\new-cosmetic-frontview\\images_ext\\INNISFREE-JEJU CHERRY BLOSSOM SKIN\n",
      "38 .\\new-cosmetic-frontview\\images_ext\\INNISFREE-NO SEBUM Mineral Powder\n",
      "52 .\\new-cosmetic-frontview\\images_ext\\INNISFREE-마이 블러셔 햇살 가득 장미\n",
      "43 .\\new-cosmetic-frontview\\images_ext\\ISOI-인텐시브 에너자이징크림\n",
      "39 .\\new-cosmetic-frontview\\images_ext\\MACQUEEN-쥬얼포텐 아이글리터\n",
      "46 .\\new-cosmetic-frontview\\images_ext\\MISSHA-레이어 블러링 모공커버 프라이머\n",
      "47 .\\new-cosmetic-frontview\\images_ext\\NEUTROGENA-HAND CREAM\n",
      "40 .\\new-cosmetic-frontview\\images_ext\\PERIPERA-핑크의순간 컬렉션 잉크 컬러 마스카라\n",
      "49 .\\new-cosmetic-frontview\\images_ext\\RIRE-BUBLE BUBLE LIP MASK\n",
      "46 .\\new-cosmetic-frontview\\images_ext\\ROKKISS-카렌듈라 에멀젼\n",
      "45 .\\new-cosmetic-frontview\\images_ext\\SENKA-PERFECT WHIP\n",
      "47 .\\new-cosmetic-frontview\\images_ext\\TONYMOLY-피키비키 아트 팝 코렉팅 베이스\n",
      "51 .\\new-cosmetic-frontview\\images_ext\\VICHY-오 떼르말 미네랄 온천수 미스트\n",
      "42 .\\new-cosmetic-office\\images_ext\\AHC-AGELESS REAL EYE CREAM FOR FACE\n",
      "46 .\\new-cosmetic-office\\images_ext\\AHC-Aura Secret Toneup Cream\n",
      "40 .\\new-cosmetic-office\\images_ext\\AHC-ONLY FOR MAN LOTION\n",
      "34 .\\new-cosmetic-office\\images_ext\\APIEU-데카소사이드 시카 겔 데이크림\n",
      "34 .\\new-cosmetic-office\\images_ext\\APIEU-스타트업 포어 프라이머\n",
      "42 .\\new-cosmetic-office\\images_ext\\BANILACO-프라임 프라이머 클래식\n",
      "32 .\\new-cosmetic-office\\images_ext\\BELIF-The true cream AQUA BOMB\n",
      "38 .\\new-cosmetic-office\\images_ext\\CLIO-스파클링 라인 프리즘 에어 아이섀도우\n",
      "32 .\\new-cosmetic-office\\images_ext\\FERRAGAMO-INCANTO CHARMS\n",
      "46 .\\new-cosmetic-office\\images_ext\\FERRARI-SCUDERIA BLACK EAU DE TOILETTE SPRAY\n",
      "34 .\\new-cosmetic-office\\images_ext\\FROMNATURE-에이지 인텐스 트리트먼트 에센스\n",
      "42 .\\new-cosmetic-office\\images_ext\\GENABELLE-LASOR SOOTHING SUNSCREEN\n",
      "26 .\\new-cosmetic-office\\images_ext\\GREEN FINGER-KIDS FACIAL LOTION\n",
      "44 .\\new-cosmetic-office\\images_ext\\HERA-BLACK CUSHION\n",
      "44 .\\new-cosmetic-office\\images_ext\\HOLIKA HOLIKA-GOOD CERA SUPER CERAMIDE MIST\n",
      "42 .\\new-cosmetic-office\\images_ext\\ILLIYOON-세라마이드 아토 수딩 젤\n",
      "36 .\\new-cosmetic-office\\images_ext\\INNISFREE-ALOE REVITAL SOOTHING GEL\n",
      "42 .\\new-cosmetic-office\\images_ext\\INNISFREE-GREEN TEA SEED ESSENC IN LOTION\n",
      "34 .\\new-cosmetic-office\\images_ext\\INNISFREE-JEJU CHERRY BLOSSOM SKIN\n",
      "38 .\\new-cosmetic-office\\images_ext\\INNISFREE-NO SEBUM Mineral Powder\n",
      "40 .\\new-cosmetic-office\\images_ext\\INNISFREE-마이 블러셔 햇살 가득 장미\n",
      "44 .\\new-cosmetic-office\\images_ext\\ISOI-인텐시브 에너자이징크림\n",
      "40 .\\new-cosmetic-office\\images_ext\\MACQUEEN-쥬얼포텐 아이글리터\n",
      "48 .\\new-cosmetic-office\\images_ext\\MISSHA-레이어 블러링 모공커버 프라이머\n",
      "44 .\\new-cosmetic-office\\images_ext\\NEUTROGENA-HAND CREAM\n",
      "42 .\\new-cosmetic-office\\images_ext\\PERIPERA-핑크의순간 컬렉션 잉크 컬러 마스카라\n",
      "36 .\\new-cosmetic-office\\images_ext\\RIRE-BUBLE BUBLE LIP MASK\n",
      "42 .\\new-cosmetic-office\\images_ext\\ROKKISS-카렌듈라 에멀젼\n",
      "36 .\\new-cosmetic-office\\images_ext\\SENKA-PERFECT WHIP\n",
      "42 .\\new-cosmetic-office\\images_ext\\TONYMOLY-피키비키 아트 팝 코렉팅 베이스\n",
      "38 .\\new-cosmetic-office\\images_ext\\VICHY-오 떼르말 미네랄 온천수 미스트\n"
     ]
    }
   ],
   "source": [
    "for ref in ['trimed-frontview', 'trimed-floor', 'original-frontview', 'original-office']:\n",
    "    ll = os.listdir(allpath['refPath'][ref])\n",
    "    for i in ll:\n",
    "        ss = os.listdir(os.path.join(allpath['refPath'][ref], i))\n",
    "        print(len(ss), os.path.join(allpath['refPath'][ref], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fe3ffff7cd4cb6984e168d54f7c448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3314:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ 0 1 ----------------\n",
      "**********classification***********\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e913ee350443e992855a2d524d24b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [n_sample]: 1 [ref]: trimed-frontview [val]: wall [knn]: True [texture]: True [threshold]: 0.83 ***********\n",
      "**********classification-bbox***********\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea4506fc1f743cc95446460b1c88339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ 0 1 ----------------\n",
      "**********classification***********\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a67a8c0e204925b0344eb9cd1a38ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [n_sample]: 1 [ref]: trimed-frontview [val]: wall [knn]: True [texture]: False [threshold]: 0.83 ***********\n",
      "**********classification-bbox***********\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d887bd4c63749b9bf994a82fda56daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ 0 1 ----------------\n",
      "**********classification***********\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937057f0f2274347a6909db9ef96d277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.00 GiB total capacity; 4.04 GiB already allocated; 20.27 MiB free; 321.06 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-525756cdeee4>\u001b[0m in \u001b[0;36mdoClassification\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_tensor3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSE_KNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtarget_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpreds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-3b56f4f1b1e7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;34m\"\"\"Extract feature vectors from input images.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;31m#         return self.backbone(images)[2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m     )\n\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.00 GiB total capacity; 4.04 GiB already allocated; 20.27 MiB free; 321.06 MiB cached)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 결과 로그파일 불러오기\n",
    "try:\n",
    "    res_df = pd.read_csv('result-log.csv')\n",
    "except:\n",
    "    res_df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "# option\n",
    "ref = 'masked-trimed'\n",
    "val = 'wall'\n",
    "# n_sample = 10\n",
    "# USE_KNN = True\n",
    "\n",
    "GET_mAP = False\n",
    "USE_TEXTURE = True\n",
    "n_slice = 3\n",
    "\n",
    "SLIDE_WINDOW = True\n",
    "# window_mode = 'first'\n",
    "\n",
    "model.threshold = 0.83\n",
    "n_rpn_box = 200\n",
    "# memo = 'randomRot,slidwindow n=%d mode=%s'%(window_size, window_mode)\n",
    "\n",
    "for i in range(5):\n",
    "    for n_sample in [1, 3, 5, 10]:\n",
    "        memo=''\n",
    "\n",
    "#             for ref in ['trimed-floor']:\n",
    "        for ref in ['trimed-frontview', 'trimed-floor', 'trimed-office', 'original-frontview', 'original-floor', 'original-office']:\n",
    "            # reference data\n",
    "            referenceImgDirPath = allpath['refPath'][ref]\n",
    "            featuresPath = 'cosmetic_features_cos.csv'\n",
    "            meanPath = 'cosmetic_means_cos.csv'\n",
    "            makeAllReferenceCSV(referenceImgDirPath, featuresPath, meanPath, n_sample) # 매번 랜덤 샘플링임\n",
    "\n",
    "            for val in ['wall', 'frontview2', 'floor2']:\n",
    "                if ref==val: continue\n",
    "                for USE_KNN in [True, False]:\n",
    "                    for USE_TEXTURE in [True, False]:\n",
    "                        print('------------------', i, n_sample, '----------------')\n",
    "\n",
    "                        # full acc용 val data\n",
    "                        imgValPath = allpath['clsValPath'][val]\n",
    "                        # 디텍션용 val data\n",
    "                        detect_ref_dir = allpath['detectionValPath'][val]\n",
    "                        class_path = allpath['classPath']\n",
    "\n",
    "                        # ---------------------------------------- do ALL\n",
    "                        target_all, preds_all = doClassification()\n",
    "                        target_all_detect, preds_all_detect, accs = doBboxClassification()\n",
    "\n",
    "                        if(GET_mAP):\n",
    "                            mAP, AP_list, fps, tps = doDetection()\n",
    "\n",
    "\n",
    "                        top1_acc = target_all.eq(preds_all[:,0]).float().mean()                 \n",
    "                        top1_acc_detect = target_all_detect.eq(preds_all_detect[:,0]).float().mean()\n",
    "                        # saveRes()-------------------------\n",
    "                        if(GET_mAP):\n",
    "                            result = [str(datetime.now()), ref, val, n_sample, USE_KNN, USE_TEXTURE, False, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "                                      top1_acc_detect.item(), mAP, memo, *AP_list, *fps, *tps, *accs]\n",
    "                        else: \n",
    "                            result = [str(datetime.now()), ref, val, n_sample, USE_KNN, USE_TEXTURE, False, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "                                  top1_acc_detect.item(), -1, memo, *([np.nan]*93), *accs]\n",
    "                        res_df = res_df.append(pd.Series(result, index=cols), ignore_index=True)\n",
    "                        res_df.to_csv('result-log.csv',index=False)\n",
    "                        res_df.to_csv('result-log.bak.csv',index=False)\n",
    "        #                     display(res_df.iloc[[-1],:])\n",
    "\n",
    "                        if(SLIDE_WINDOW):   \n",
    "                            # window slide 보정\n",
    "                            new_preds_all = calcWindowSlidePreds(target_all, preds_all, n_window=7, mode='first')\n",
    "                            new_preds_all_detect = calcWindowSlidePreds(target_all_detect, preds_all_detect, n_window=7, mode='first')\n",
    "                            top1_acc = target_all.eq(torch.tensor(new_preds_all)).float().mean()\n",
    "                            top1_acc_detect = target_all_detect.eq(torch.tensor(new_preds_all_detect)).float().mean()    \n",
    "                            # saveRes()------------------------\n",
    "                            if(GET_mAP):\n",
    "                                result = [str(datetime.now()), ref, val, n_sample, USE_KNN, USE_TEXTURE, SLIDE_WINDOW, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "                                          top1_acc_detect.item(), mAP, memo, *AP_list, *fps, *tps, *accs]\n",
    "                            else: \n",
    "                                result = [str(datetime.now()), ref, val, n_sample, USE_KNN, USE_TEXTURE, SLIDE_WINDOW, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "                                      top1_acc_detect.item(), -1, memo, *([np.nan]*93), *accs]\n",
    "                            res_df = res_df.append(pd.Series(result, index=cols), ignore_index=True)\n",
    "                            res_df.to_csv('result-log.csv',index=False)\n",
    "                            res_df.to_csv('result-log.bak.csv',index=False)\n",
    "    #                                 display(res_df.iloc[[-1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 28)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2862\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-58-6018076c1bc9>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', \"SLIDE_WINDOW = True\\n# option\\nref = 'no-masked-trimed'\\nval = 'wall'\\nn_sample = 10\\nUSE_KNN = True\\nmemo = 'slidwindow n=3, mode=all'\\nmodel.threshold = 0.83\\nn_rpn_box = 200\\n\\n# reference data\\nreferenceImgDirPath = allpath['refPath'][ref]\\n# full acc용 val data\\nimgValPath = allpath['clsValPath'][val]\\n# 디텍션용 val data\\ndetect_ref_dir = allpath['detectionValPath'][val]\\nclass_path = allpath['classPath']\\n\\nfor USE_KNN in [True, False]:\\n    top1_acc = doClassification(SLIDE_WINDOW)\\n    top1_acc_detect = doBboxClassification(SLIDE_WINDOW)\\n#     mAP, AP_list, fps, tps = doDetection(SLIDE_WINDOW)\\n    \\n#     result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\\n#               top1_acc_detect.item(), mAP, memo, *AP_list, *fps, *tps]\\n    result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\\n              top1_acc_detect.item(), -1, memo, *([np.nan]*93)]\\n    res_df = res_df.append(pd.Series(result, index=cols), ignore_inda%%time\\n# option\\nref = 'masked-trimed'\\nval = 'wall'\\nn_sample = 10\\nUSE_KNN = True\\n\\nUSE_TEXTURE = False\\nn_slice = 3\\n\\nSLIDE_WINDOW = True\\nwindow_size = 7\\nwindow_mode = 'first'\\n\\nmodel.threshold = 0.83\\nn_rpn_box = 200\\nmemo = 'randomRot,slidwindow n=%d mode=%s'%(window_size, window_mode)\\n\\n# for n_sample in [10, 20, 30, 5]:\\n# memo = 'no-slide'\\n\\nfor ref in ['no-masked-trimed', 'original', 'full']:\\n    # reference data\\n    referenceImgDirPath = allpath['refPath'][ref]\\n    featuresPath = 'cosmetic_features_cos.csv'\\n    meanPath = 'cosmetic_means_cos.csv'\\n    makeAllReferenceCSV(referenceImgDirPath, featuresPath, meanPath, n_sample) # 매번 랜덤 샘플링임\\n    \\n    for window_size in [7,6,5,4]:\\n        memo = 'slidwindow n=%d mode=%s'%(window_size, window_mode)\\n\\n        for val in ['wall', 'frontview2', 'floor2']:\\n    #         for val in ['floor2']:\\n            if ref==val: continue\\n            for USE_KNN in [True, False]:\\n\\n                # full acc용 val data\\n                imgValPath = allpath['clsValPath'][val]\\n                # 디텍션용 val data\\n                detect_ref_dir = allpath['detectionValPath'][val]\\n                class_path = allpath['classPath']\\n\\n                # ---------------------------------------- do ALL\\n                top1_acc = doClassification(SLIDE_WINDOW, n_window=window_size, mode=window_mode)\\n                top1_acc_detect = doBboxClassification(SLIDE_WINDOW, n_window=window_size, mode=window_mode)\\n    #                 mAP, AP_list, fps, tps = doDetection(SLIDE_WINDOW)\\n\\n    #                 result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\\n    #                           top1_acc_detect.item(), mAP, memo, *AP_list, *fps, *tps]\\n                result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\\n                          top1_acc_detect.item(), -1, memo, *([np.nan]*93)]\\n                res_df = res_df.append(pd.Series(result, index=cols), ignore_index=True)\\n                res_df.to_csv('result-log.csv',index=False)\\n                res_df.to_csv('result-log.bak.csv',index=False)\\n                display(res_df.iloc[[-1],:])ex=True)\\n    res_df.to_csv('result-log.csv',index=False)\\n    res_df.to_csv('result-log.bak.csv',index=False)\\n    display(res_df.iloc[[-1],:])\")\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2103\u001b[0m, in \u001b[0;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[0;32m\"<decorator-gen-63>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35mtime\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\"\u001b[0m, line \u001b[0;32m187\u001b[0m, in \u001b[0;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\"\u001b[0m, line \u001b[0;32m1179\u001b[0m, in \u001b[0;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\"\u001b[1;36m, line \u001b[1;32m99\u001b[1;36m, in \u001b[1;35mast_parse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    res_df = res_df.append(pd.Series(result, index=cols), ignore_inda%%time\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 결과 로그파일 불러오기\n",
    "# try:\n",
    "#     res_df = pd.read_csv('result-log.csv')\n",
    "# except:\n",
    "#     res_df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "# # option\n",
    "# ref = 'masked-trimed'\n",
    "# val = 'wall'\n",
    "# n_sample = 10\n",
    "# USE_KNN = True\n",
    "\n",
    "# GET_mAP = True\n",
    "# USE_TEXTURE = False\n",
    "# n_slice = 3\n",
    "\n",
    "# SLIDE_WINDOW = True\n",
    "# window_size = 7\n",
    "# window_mode = 'first'\n",
    "\n",
    "# model.threshold = 0.83\n",
    "# n_rpn_box = 200\n",
    "# # memo = 'randomRot,slidwindow n=%d mode=%s'%(window_size, window_mode)\n",
    "\n",
    "# # for i in range(100):\n",
    "# #     print('---------', i, '----------')\n",
    "# #     for n_sample in [30]:\n",
    "# #         memo = 'finding sota'\n",
    "\n",
    "# # for USE_TEXTURE in [True, False]:\n",
    "# #     if(USE_TEXTURE): memo = \"Texture \"+memo\n",
    "# for n_sample in [5, 10, 20, 30]:\n",
    "#     memo = '' # 실행전에 트랜스포머 바꿀것\n",
    "\n",
    "#     for ref in ['no-masked-trimed', 'original', 'full']:\n",
    "#         # reference data\n",
    "#         referenceImgDirPath = allpath['refPath'][ref]\n",
    "#         featuresPath = 'cosmetic_features_cos.csv'\n",
    "#         meanPath = 'cosmetic_means_cos.csv'\n",
    "#         makeAllReferenceCSV(referenceImgDirPath, featuresPath, meanPath, n_sample) # 매번 랜덤 샘플링임\n",
    "\n",
    "#         for val in ['wall', 'frontview2', 'floor2']:\n",
    "#             if ref==val: continue\n",
    "#             for USE_KNN in [True, False]:\n",
    "\n",
    "#                 # full acc용 val data\n",
    "#                 imgValPath = allpath['clsValPath'][val]\n",
    "#                 # 디텍션용 val data\n",
    "#                 detect_ref_dir = allpath['detectionValPath'][val]\n",
    "#                 class_path = allpath['classPath']\n",
    "\n",
    "#                 # ---------------------------------------- do ALL\n",
    "#                 target_all, preds_all = doClassification()\n",
    "#                 target_all_detect, preds_all_detect = doBboxClassification()\n",
    "\n",
    "#                 if(GET_mAP):\n",
    "#                     mAP, AP_list, fps, tps = doDetection()\n",
    "\n",
    "\n",
    "#                 top1_acc = target_all.eq(preds_all[:,0]).float().mean()                 \n",
    "#                 top1_acc_detect = target_all_detect.eq(preds_all_detect[:,0]).float().mean()\n",
    "#                 # saveRes()-------------------------\n",
    "#                 if(GET_mAP):\n",
    "#                     result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "#                               top1_acc_detect.item(), mAP, memo, *AP_list, *fps, *tps]\n",
    "#                 else: \n",
    "#                     result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "#                           top1_acc_detect.item(), -1, memo, *([np.nan]*93)]\n",
    "#                 res_df = res_df.append(pd.Series(result, index=cols), ignore_index=True)\n",
    "#                 res_df.to_csv('result-log.csv',index=False)\n",
    "#                 res_df.to_csv('result-log.bak.csv',index=False)\n",
    "# #                     display(res_df.iloc[[-1],:])\n",
    "\n",
    "#                 if(SLIDE_WINDOW):   \n",
    "#                     for window_mode in ['first', 'all']:\n",
    "#                         for window_size in [3,4,5,6,7]:\n",
    "#                             # window slide 보정\n",
    "#                             _memo = memo + ' slidewindow n=%d mode=%s'%(window_size, window_mode)  \n",
    "#                             _memo = _memo.strip()\n",
    "#                             new_preds_all = calcWindowSlidePreds(target_all, preds_all, n_window=window_size, mode=window_mode)\n",
    "#                             new_preds_all_detect = calcWindowSlidePreds(target_all_detect, preds_all_detect, n_window=window_size, mode=window_mode)\n",
    "#                             top1_acc = target_all.eq(torch.tensor(new_preds_all)).float().mean()\n",
    "#                             top1_acc_detect = target_all_detect.eq(torch.tensor(new_preds_all_detect)).float().mean()    \n",
    "#                             # saveRes()------------------------\n",
    "#                             if(GET_mAP):\n",
    "#                                 result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "#                                           top1_acc_detect.item(), mAP, _memo, *AP_list, *fps, *tps]\n",
    "#                             else: \n",
    "#                                 result = [str(datetime.now()), ref, val, n_sample, USE_KNN, model.threshold, n_rpn_box, top1_acc.item(),\n",
    "#                                       top1_acc_detect.item(), -1, _memo, *([np.nan]*93)]\n",
    "#                             res_df = res_df.append(pd.Series(result, index=cols), ignore_index=True)\n",
    "#                             res_df.to_csv('result-log.csv',index=False)\n",
    "#                             res_df.to_csv('result-log.bak.csv',index=False)\n",
    "# #                                 display(res_df.iloc[[-1],:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
